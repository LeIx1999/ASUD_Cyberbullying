{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873acbb0",
   "metadata": {},
   "source": [
    "### Download german spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b23a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-lg==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.3.0/de_core_news_lg-3.3.0-py3-none-any.whl (567.8 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m447.6/567.8 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abb67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import re\n",
    "import umap\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/jannis/ASUD_Cyberbullying/prepared_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.granulareKlassifikation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9301c1",
   "metadata": {},
   "source": [
    "## Preprocess the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in tweet as one string \n",
    "data = df \n",
    "data[\"tweets_clean\"] = [\" \".join(eval(word)) for word in data[\"Wörter in Tweet\"]] \n",
    "\n",
    "# drop unicode emojis and LBR\n",
    "new_tweet_ls = []\n",
    "for tweet in data.tweets_clean:\n",
    "    tweet_sp = tweet.split()\n",
    "    for word in tweet_sp:\n",
    "        if any(x in word for x in [\"000\", \"LBR\", \"lbr\"]) :\n",
    "            tweet = tweet.replace(word, \"\")\n",
    "        \n",
    "    new_tweet_ls.append(tweet)\n",
    "\n",
    "data[\"tweets_clean\"] = new_tweet_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the tweets\n",
    "def process_tweets(tweet : str):\n",
    "    # all lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # remove Sonderzeichen etc (based on Sonderzeichen.txt)\n",
    "    tweet = re.sub('[^a-zA-ZäöüÄÖÜß]', \" \", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in training and test\n",
    "data_train = data.sample(round(0.75 * len(data)), random_state = 1).reset_index()\n",
    "data_test = data[~data.index.isin(data_train.index)].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ed127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data_train\n",
    "data_pre[\"tweets_clean\"] = [process_tweets(tweet) for tweet in data_pre[\"tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06592f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eccecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spacy docs with a nlp pipeline\n",
    "# https://spacy.io/usage/processing-pipelines\n",
    "tweet_docs = [nlp(tweet) for tweet in data_pre[\"tweets_clean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ea102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the generated lemmas for words that are no stopwords and have a length of more than two\n",
    "# Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced\n",
    "# form belongs to the language. This reduced form or root word is called a lemma.\n",
    "tweet_words = [\n",
    "    [\n",
    "    word.lemma_ for word in doc if ((not word.is_stop) and (len(word) >= 3))\n",
    "    ]\n",
    "    for doc in tweet_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad3ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model(\"/Users/jannis/cc.de.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensions\n",
    "fasttext.util.reduce_model(ft, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the pre-trained vectors\n",
    "ft.get_nearest_neighbors('CDU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75646765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_word_vector(\"Ferkel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tweet_words = []\n",
    "test = [uni_tweet_words.extend(tweet) for tweet in tweet_words]\n",
    "len(uni_tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184304d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only words that appear at least 3 times\n",
    "from collections import Counter\n",
    "words_counter = Counter(uni_tweet_words)\n",
    "words_counter = {k: v for k, v in words_counter.items() if v >= 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2dd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tweet_words = list(set(uni_tweet_words))\n",
    "uni_tweet_words = [word for word in uni_tweet_words if word in words_counter.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbbdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uni_tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets where there are no words in the fasttext model\n",
    "def remove_nan_tweets(tweet_words, model):\n",
    "    tweet_words_dict = {}\n",
    "    for tweet in tweet_words:\n",
    "        sum_occurr = 0\n",
    "        for word in tweet:\n",
    "            if word in model:\n",
    "                sum_occurr +=1\n",
    "        if sum_occurr > 0:\n",
    "            tweet_words_dict[tweet_words.index(tweet)] = True\n",
    "\n",
    "    return tweet_words_dict\n",
    "\n",
    "tweet_words_dict = remove_nan_tweets(tweet_words, uni_tweet_words)\n",
    "    \n",
    "# subset data_pre and tweet_words, only tweets where at least one word is in the word2vec model\n",
    "data_pre = data_pre.iloc[list(tweet_words_dict.keys()), :]\n",
    "data_pre = data_pre.reset_index()\n",
    "tweet_words = [tweet_words[i] for i in list(tweet_words_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac80040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a category to every word\n",
    "category_dict = {}\n",
    "for word in uni_tweet_words:\n",
    "    max_numb = {}\n",
    "    for cat in data_pre[\"granulareKlassifikation\"].unique():\n",
    "        count = sum([tweet.count(word.lower()) for tweet in data_pre[data_pre[\"granulareKlassifikation\"] == cat][\"tweet\"]])\n",
    "        max_numb[cat] = count   \n",
    "    category_dict[word] = max(max_numb, key = max_numb.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a547ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a category to every word (percentage)\n",
    "category_dict_per = {}\n",
    "for word in uni_tweet_words:\n",
    "    max_numb = {}\n",
    "    for cat in data_pre[\"granulareKlassifikation\"].unique():\n",
    "        count = sum([tweet.count(word.lower()) for tweet in data_pre[data_pre[\"granulareKlassifikation\"] == cat][\"tweet\"]]) / len(data_pre[data_pre[\"granulareKlassifikation\"] == cat][\"tweet\"])\n",
    "        max_numb[cat] = count   \n",
    "    category_dict_per[word] = max(max_numb, key = max_numb.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32982aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution (count)\n",
    "category_dict\n",
    "Counter(category_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution (percentage)\n",
    "category_dict\n",
    "Counter(category_dict_per.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334bb05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_pre[\"granulareKlassifikation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the word vectors\n",
    "# reduce to two dimensions\n",
    "reducer = umap.UMAP(metric='cosine', n_components=2, n_neighbors=15, min_dist=0.00, random_state=0)\n",
    "\n",
    "# create word vectors for all words in uni_tweet_words\n",
    "X = np.empty([2, 2])\n",
    "X = [np.append(X, ft.get_word_vector(word)) for word in uni_tweet_words]\n",
    "X = np.asarray(X)\n",
    "\n",
    "# transform to two dimensions\n",
    "embedding = reducer.fit_transform(X[:,:])\n",
    "\n",
    "# plot projected word vectors\n",
    "plt.figure(figsize=(4,4), dpi=300)\n",
    "\n",
    "# scatterplot of projected word vectors\n",
    "colors = {\"INSULT\": '#eb4034',\n",
    "          \"OTHER\": \"#dbd635\",\n",
    "          \"ABUSE\": '#34b1eb',\n",
    "          \"PROFANITY\": \"#52ab32\"}\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c = [colors.get(cat, \" \") for cat in list(category_dict.values())[:]],\n",
    "    s = 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we should plot this with 3 dimensions \n",
    "# Project into 3D space\n",
    "reducer3D = umap.UMAP(metric='cosine', n_components=3, n_neighbors=15, min_dist=0.00, random_state=0)\n",
    "embedding3D = reducer3D.fit_transform(X[:,:])\n",
    "\n",
    "\n",
    "# plot in 3D and create animation (rotate space)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(4,4), dpi=100)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(\n",
    "    embedding3D[:, 0],\n",
    "    embedding3D[:, 1],\n",
    "    embedding3D[:, 2],\n",
    "    c = [colors.get(cat, \" \") for cat in list(category_dict_per.values())[:]],\n",
    "    s = 2\n",
    "    )\n",
    "for color in ['#eb4034',\"#dbd635\",'#34b1eb',\"#52ab32\"]:\n",
    "    lbl = list(colors.keys())[list(colors.values()).index(color)]\n",
    "    plt.scatter([],[], [], color=color, label=lbl)\n",
    "plt.legend()\n",
    "\n",
    "def rotate(angle):\n",
    "    ax.view_init(azim=angle)\n",
    "\n",
    "rot_animation = animation.FuncAnimation(fig, rotate, frames=np.arange(0, 362,10), repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db6f91",
   "metadata": {},
   "source": [
    "## Classify new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate center of mass vector for list of words (used here for article as\n",
    "# collection of words)\n",
    "def get_com_vector(words : list) -> np.array:\n",
    "    # list of words in the word2vec model\n",
    "    words = [word for word in words if word in uni_tweet_words]\n",
    "    # get the vectors\n",
    "    vectors = np.array([ft.get_word_vector(word) for word in words])\n",
    "    # return the sum of all vectors devided by the amount of words from words in the model\n",
    "    vector = np.sum(vectors, axis=0)\n",
    "    return vector / len(words)\n",
    "\n",
    "# get vector for each tweet\n",
    "tweet_vectors = []\n",
    "for tweet in tweet_words:\n",
    "    vec = get_com_vector(tweet)\n",
    "    tweet_vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58748b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess and transform new tweets\n",
    "def new_tweet_vector(tweet : str):\n",
    "    prep_new = process_tweets(tweet)\n",
    "    doc_new = nlp(prep_new)\n",
    "\n",
    "    words_new = [\n",
    "                    word.lemma_ for word in doc_new\n",
    "                    if (not word.is_stop) and (len(word)>2)\n",
    "                 ]\n",
    "\n",
    "    # calculate vector for new article\n",
    "    new_tweet_v = get_com_vector(words_new)\n",
    "\n",
    "    return new_tweet_v, words_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede5a08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create tweet vectors of test tweets\n",
    "new_tweet_v = []\n",
    "words_new = []\n",
    "for tweet in data_test.tweets_clean:\n",
    "    new_tweet_v.append(new_tweet_vector(tweet)[0])\n",
    "    words_new.append(new_tweet_vector(tweet)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d82a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets where there are no words in the word2vec model  \n",
    "tweet_words_dict = remove_nan_tweets(words_new, uni_tweet_words)   \n",
    "\n",
    "# subset data_test and tweet_words, only tweets where at least one word is in the word2vec model\n",
    "data_test = data_test.iloc[list(tweet_words_dict.keys()), :]\n",
    "data_test = data_test.reset_index()\n",
    "new_tweet_v = [new_tweet_v[i] for i in list(tweet_words_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the classes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y_transformed = encoder.fit_transform(data_pre[\"granulareKlassifikation\"])\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(tweet_vectors, y_transformed)\n",
    "rfc_predictions = rfc.predict(new_tweet_v)\n",
    "# rfc_human_readeable = encoder.inverse_transform(rfc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform predictions to classes\n",
    "encoder_dict = dict(enumerate(encoder.classes_.flatten(), 0))\n",
    "rfc_predictions = [encoder_dict[x] for x in rfc_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "acc = 0\n",
    "for i in range(len(rfc_predictions)):\n",
    "    if rfc_predictions[i] == data_test[\"granulareKlassifikation\"][i]:\n",
    "        acc += 1\n",
    "accuracy = acc / len(rfc_predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf63eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(data_test[\"granulareKlassifikation\"], rfc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88a31c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93621071",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"granulareKlassifikation\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts of predictions\n",
    "[cat_predictions.count(x) for x in ['ABUSE', 'PROFANITY','INSULT', 'OTHER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"granulareKlassifikation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f6887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
