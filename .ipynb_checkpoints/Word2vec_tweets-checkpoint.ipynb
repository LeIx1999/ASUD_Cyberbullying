{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873acbb0",
   "metadata": {},
   "source": [
    "### Download german spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b23a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23abb67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannis/opt/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import re\n",
    "import umap\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca4f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/jannis/ASUD_Cyberbullying/prepared_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac4acb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>binaereKlassifikation</th>\n",
       "      <th>granulareKlassifikation</th>\n",
       "      <th>Wörter in Tweet</th>\n",
       "      <th>Wörter pro Tweet</th>\n",
       "      <th>Tweets ohne Satzzeichen</th>\n",
       "      <th>Wortlänge</th>\n",
       "      <th>Durchschnittliche Wortlänge</th>\n",
       "      <th>Verlinkungen</th>\n",
       "      <th>Verlinkung pro Tweet</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Hashtags pro Tweet</th>\n",
       "      <th>utf Codes</th>\n",
       "      <th>utf Codes pro Tweet</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Emojis pro Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JanZimmHHB @mopo Komisch das die Realitätsver...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>['komisch', 'das', 'die', 'realitätsverweigeru...</td>\n",
       "      <td>11</td>\n",
       "      <td>['Komisch', 'das', 'die', 'Realitätsverweigeru...</td>\n",
       "      <td>[7, 3, 3, 21, 5, 3, 6, 12, 9, 8, 7]</td>\n",
       "      <td>7.64</td>\n",
       "      <td>['@JanZimmHHB', '@mopo']</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@faznet @Gruene_Europa @SPDEuropa @CDU CDU ste...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>['cdu', 'steht', 'seid', 'strauss,', 'kohl,', ...</td>\n",
       "      <td>13</td>\n",
       "      <td>['CDU', 'steht', 'seid', 'Strauss', 'Kohl', 'S...</td>\n",
       "      <td>[3, 5, 4, 7, 4, 8, 7, 3, 10, 11, 3, 4, 7]</td>\n",
       "      <td>5.85</td>\n",
       "      <td>['@faznet', '@Gruene_Europa', '@SPDEuropa', '@...</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DLFNachrichten Die Gesichter, Namen, Religion...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>['die', 'gesichter,', 'namen,', 'religion', 'd...</td>\n",
       "      <td>10</td>\n",
       "      <td>['Die', 'Gesichter', 'Namen', 'Religion', 'der...</td>\n",
       "      <td>[3, 9, 5, 8, 3, 5, 5, 3, 10, 9]</td>\n",
       "      <td>6.00</td>\n",
       "      <td>['@DLFNachrichten']</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@welt Wie verwirrt muss man sein um sich zu we...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>['wie', 'verwirrt', 'muss', 'man', 'sein', 'um...</td>\n",
       "      <td>28</td>\n",
       "      <td>['Wie', 'verwirrt', 'muss', 'man', 'sein', 'um...</td>\n",
       "      <td>[3, 8, 4, 3, 4, 2, 4, 2, 7, 3, 9, 12, 3, 6, 4,...</td>\n",
       "      <td>5.43</td>\n",
       "      <td>['@welt']</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@hacker_1991 @torben_braga Weil die AfD den Fe...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>['weil', 'die', 'afd', 'den', 'ferkelfunk', 'a...</td>\n",
       "      <td>30</td>\n",
       "      <td>['Weil', 'die', 'AfD', 'den', 'Ferkelfunk', 'a...</td>\n",
       "      <td>[4, 3, 3, 3, 10, 10, 4, 7, 5, 3, 6, 6, 11, 5, ...</td>\n",
       "      <td>5.17</td>\n",
       "      <td>['@hacker_1991', '@torben_braga']</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet binaereKlassifikation  \\\n",
       "0  @JanZimmHHB @mopo Komisch das die Realitätsver...               OFFENSE   \n",
       "1  @faznet @Gruene_Europa @SPDEuropa @CDU CDU ste...               OFFENSE   \n",
       "2  @DLFNachrichten Die Gesichter, Namen, Religion...                 OTHER   \n",
       "3  @welt Wie verwirrt muss man sein um sich zu we...               OFFENSE   \n",
       "4  @hacker_1991 @torben_braga Weil die AfD den Fe...               OFFENSE   \n",
       "\n",
       "  granulareKlassifikation                                    Wörter in Tweet  \\\n",
       "0                  INSULT  ['komisch', 'das', 'die', 'realitätsverweigeru...   \n",
       "1                   ABUSE  ['cdu', 'steht', 'seid', 'strauss,', 'kohl,', ...   \n",
       "2                   OTHER  ['die', 'gesichter,', 'namen,', 'religion', 'd...   \n",
       "3                   ABUSE  ['wie', 'verwirrt', 'muss', 'man', 'sein', 'um...   \n",
       "4                   ABUSE  ['weil', 'die', 'afd', 'den', 'ferkelfunk', 'a...   \n",
       "\n",
       "   Wörter pro Tweet                            Tweets ohne Satzzeichen  \\\n",
       "0                11  ['Komisch', 'das', 'die', 'Realitätsverweigeru...   \n",
       "1                13  ['CDU', 'steht', 'seid', 'Strauss', 'Kohl', 'S...   \n",
       "2                10  ['Die', 'Gesichter', 'Namen', 'Religion', 'der...   \n",
       "3                28  ['Wie', 'verwirrt', 'muss', 'man', 'sein', 'um...   \n",
       "4                30  ['Weil', 'die', 'AfD', 'den', 'Ferkelfunk', 'a...   \n",
       "\n",
       "                                           Wortlänge  \\\n",
       "0                [7, 3, 3, 21, 5, 3, 6, 12, 9, 8, 7]   \n",
       "1          [3, 5, 4, 7, 4, 8, 7, 3, 10, 11, 3, 4, 7]   \n",
       "2                    [3, 9, 5, 8, 3, 5, 5, 3, 10, 9]   \n",
       "3  [3, 8, 4, 3, 4, 2, 4, 2, 7, 3, 9, 12, 3, 6, 4,...   \n",
       "4  [4, 3, 3, 3, 10, 10, 4, 7, 5, 3, 6, 6, 11, 5, ...   \n",
       "\n",
       "   Durchschnittliche Wortlänge  \\\n",
       "0                         7.64   \n",
       "1                         5.85   \n",
       "2                         6.00   \n",
       "3                         5.43   \n",
       "4                         5.17   \n",
       "\n",
       "                                        Verlinkungen  Verlinkung pro Tweet  \\\n",
       "0                           ['@JanZimmHHB', '@mopo']                     2   \n",
       "1  ['@faznet', '@Gruene_Europa', '@SPDEuropa', '@...                     4   \n",
       "2                                ['@DLFNachrichten']                     1   \n",
       "3                                          ['@welt']                     1   \n",
       "4                  ['@hacker_1991', '@torben_braga']                     2   \n",
       "\n",
       "  Hashtags  Hashtags pro Tweet utf Codes  utf Codes pro Tweet Emojis  \\\n",
       "0       []                   0        []                    0     []   \n",
       "1       []                   0        []                    0     []   \n",
       "2       []                   0        []                    0     []   \n",
       "3       []                   0        []                    0     []   \n",
       "4       []                   0        []                    0     []   \n",
       "\n",
       "   Emojis pro Tweet  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa3944f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER        2061\n",
       "INSULT        459\n",
       "ABUSE         400\n",
       "PROFANITY     111\n",
       "Name: granulareKlassifikation, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.granulareKlassifikation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9301c1",
   "metadata": {},
   "source": [
    "## Wordclouds for every Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ddd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud without stopwords\n",
    "data = df\n",
    "german_stopwords = stopwords.words(\"german\")\n",
    "sw = set(STOPWORDS)\n",
    "\n",
    "# drop unicode emojis\n",
    "new_tweet_ls = []\n",
    "for tweet in data.tweet:\n",
    "    tweet_sp = tweet.split()\n",
    "    for word in tweet_sp:\n",
    "        if \"000\" in word:\n",
    "            tweet = tweet.replace(word, \"\")\n",
    "    new_tweet_ls.append(tweet)\n",
    "\n",
    "data_wc = data\n",
    "data_wc[\"tweet\"] = new_tweet_ls\n",
    "\n",
    "# Create a word cloud for every category\n",
    "wc_OTHER = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                     stopwords=german_stopwords).generate(str(data_wc[data_wc[\"binaereKlassifikation\"] == \"OTHER\"][\"tweet\"].values))\n",
    "wc_OFFENSE = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data_wc[data_wc[\"binaereKlassifikation\"] == \"OFFENSE\"][\"tweet\"].values))\n",
    "wc_insult = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data_wc[data_wc[\"granulareKlassifikation\"] == \"INSULT\"][\"tweet\"].values))\n",
    "wc_abuse = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data_wc[data_wc[\"granulareKlassifikation\"] == \"ABUSE\"][\"tweet\"].values))\n",
    "wc_profanity = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data_wc[data_wc[\"granulareKlassifikation\"] == \"PROFANITY\"][\"tweet\"].values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad9354",
   "metadata": {},
   "source": [
    "### OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2126b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot word clouds\n",
    "plt.imshow(wc_OTHER, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud OTHER\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113216a",
   "metadata": {},
   "source": [
    "### OFFENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7604e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(wc_OFFENSE, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud OFFENSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c524147",
   "metadata": {},
   "source": [
    "### INSULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd46b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(wc_insult, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud INSULT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d1b78",
   "metadata": {},
   "source": [
    "### ABUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wc_abuse, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud ABUSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08735a3b",
   "metadata": {},
   "source": [
    "### PROFANITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wc_profanity, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud PROFANITY\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the tweets\n",
    "def process_tweets(tweet : str):\n",
    "    # all lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # remove Sonderzeichen etc (based on Sonderzeichen.txt)\n",
    "    tweet = re.sub('[^a-zA-ZäöüÄÖÜß]', \" \", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in training and test\n",
    "data_train = data.sample(round(0.75 * len(data)), random_state = 1).reset_index()\n",
    "data_test = data[~data.index.isin(data_train.index)].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ed127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data_train\n",
    "data_pre[\"tweet\"] = [process_tweets(tweet) for tweet in data_pre[\"tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06592f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eccecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spacy docs with a nlp pipeline\n",
    "tweet_docs = [nlp(tweet) for tweet in data_pre[\"tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ea102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the generated lemmas for words that are no stopwords and have a length of more than two\n",
    "tweet_words = [\n",
    "    [\n",
    "    word.lemma_ for word in doc if ((not word.is_stop) and (len(word) >= 3))\n",
    "    ]\n",
    "    for doc in tweet_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6bfb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word2vec model\n",
    "word2vec = Word2Vec(tweet_words, min_count=3, sg=1, hs=0, negative=9,\n",
    "                    ns_exponent=0.69, window=10, vector_size=60, epochs=80)\n",
    "\n",
    "# train the model\n",
    "word2vec.train(tweet_words, total_examples=word2vec.corpus_count, epochs=word2vec.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9639514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test word similarities\n",
    "def test_word_sim(word_a, word_b):\n",
    "    print(f'{word_a} | {word_b}: {word2vec.wv.similarity(word_a, word_b)}')\n",
    "    \n",
    "    \n",
    "test_word_sim(\"CDU\", \"SPD\")\n",
    "test_word_sim(\"Korruption\", \"Europa\")\n",
    "test_word_sim(\"Afd\", \"Führer\")\n",
    "test_word_sim(\"Flüchtling\", \"Terror\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2 word similarities\n",
    "def test2_word_sim(word_list):\n",
    "    print(f'{word_list}: {word2vec.wv.most_similar(positive=word_list, negative=[], topn = 5)}')\n",
    "    \n",
    "test2_word_sim([\"Deutschland\", \"Merkel\"])\n",
    "test2_word_sim([\"Flüchtling\", \"Deutschland\"])\n",
    "test2_word_sim([\"Polizei\"])\n",
    "test2_word_sim([\"Feiertag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_word_sim([\"Geld\"])\n",
    "\n",
    "test2_word_sim([\"Welt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac80040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a category to every word\n",
    "category_dict = {}\n",
    "for word in word2vec.wv.index_to_key:\n",
    "    max_numb = {}\n",
    "    for cat in data_pre[\"granulareKlassifikation\"].unique():\n",
    "        count = sum([tweet.count(word.lower()) for tweet in data_pre[data_pre[\"granulareKlassifikation\"] == cat][\"tweet\"]])\n",
    "        max_numb[cat] = count   \n",
    "    category_dict[word] = max(max_numb, key = max_numb.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32982aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution\n",
    "from collections import Counter\n",
    "category_dict\n",
    "Counter(category_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we should look at percentages?\n",
    "data_pre[\"granulareKlassifikation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the word vectors\n",
    "# reduce to two dimensions\n",
    "reducer = umap.UMAP(metric='cosine', n_components=2, n_neighbors=15, min_dist=0.00, random_state=0)\n",
    "\n",
    "# X holds the word vectors for all words in vocabulary\n",
    "X = word2vec.syn1neg\n",
    "\n",
    "# transform to two dimensions\n",
    "embedding = reducer.fit_transform(X[:,:])\n",
    "\n",
    "# plot projected word vectors\n",
    "plt.figure(figsize=(4,4), dpi=300)\n",
    "\n",
    "# scatterplot of projected word vectors\n",
    "colors = {\"INSULT\": '#eb4034',\n",
    "          \"OTHER\": \"#dbd635\",\n",
    "          \"ABUSE\": '#34b1eb',\n",
    "          \"PROFANITY\": \"#52ab32\"}\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c = [colors.get(cat, \" \") for cat in list(category_dict.values())[:]],\n",
    "    s = 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we should plot this with 3 dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db6f91",
   "metadata": {},
   "source": [
    "## Classify new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate center of mass vector for list of words (used here for article as\n",
    "# collection of words)\n",
    "def get_com_vector(words : list) -> np.array:\n",
    "    # list of words in the word2vec model\n",
    "    words = [word for word in words if word in word2vec.wv.index_to_key]\n",
    "    # get the vectors\n",
    "    vectors = np.array([word2vec.wv.get_vector(word) for word in words])\n",
    "    # return the sum of all vectors devided by the amount of words from words in the model\n",
    "    vector = np.sum(vectors, axis=0)\n",
    "    return vector / len(words)\n",
    "\n",
    "# get vector for each tweet\n",
    "tweet_vectors = []\n",
    "for word in tweet_words:\n",
    "    vec = get_com_vector(word)\n",
    "    tweet_vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55589bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similartiy between two vectors\n",
    "def cos_sim(vec0: np.array, vec1: np.array) -> float:\n",
    "    return np.dot(vec0, vec1)/(np.linalg.norm(vec0)*np.linalg.norm(vec1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85154680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a vector by comparing cosine similarity to known article vectors\n",
    "def classify_tweet(tweet_vectors: np.array, new_tweet_vector: np.array):\n",
    "\n",
    "    # split tweet_vectors into categorys\n",
    "    tweet_vectors_insult = [\n",
    "                                  tweet_vectors[i]\n",
    "                                  for i in range(len(tweet_vectors))\n",
    "                                  if (data_pre[\"granulareKlassifikation\"][i]=='INSULT')\n",
    "                                ]\n",
    "\n",
    "    tweet_vectors_abuse = [\n",
    "                               tweet_vectors[i]\n",
    "                               for i in range(len(tweet_vectors))\n",
    "                               if (data_pre[\"granulareKlassifikation\"][i]=='ABUSE')\n",
    "                            ]\n",
    "    \n",
    "    tweet_vectors_other = [\n",
    "                               tweet_vectors[i]\n",
    "                               for i in range(len(tweet_vectors))\n",
    "                               if (data_pre[\"granulareKlassifikation\"][i]=='OTHER')\n",
    "                            ]\n",
    "    tweet_vectors_profanity = [\n",
    "                               tweet_vectors[i]\n",
    "                               for i in range(len(tweet_vectors))\n",
    "                               if (data_pre[\"granulareKlassifikation\"][i]=='PROFANITY')\n",
    "                            ]\n",
    "    # calculate similarities between new_article_vector and known\n",
    "    # article_vectors for all categories\n",
    "    sims_insult = [\n",
    "                    cos_sim(new_tweet_vector, av)\n",
    "                    for av in tweet_vectors_insult\n",
    "                ]\n",
    "    \n",
    "    # drop nas\n",
    "    sims_insult = [x for x in sims_insult if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    sims_abuse = [\n",
    "                    cos_sim(new_tweet_vector, av)\n",
    "                    for av in tweet_vectors_abuse\n",
    "                 ]\n",
    "    # drop nas\n",
    "    sims_abuse = [x for x in sims_abuse if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    sims_other = [\n",
    "                    cos_sim(new_tweet_vector, av)\n",
    "                    for av in tweet_vectors_other\n",
    "                 ]\n",
    "    # drop nas\n",
    "    sims_other = [x for x in sims_other if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    sims_profanity = [\n",
    "                    cos_sim(new_tweet_vector, av)\n",
    "                    for av in tweet_vectors_profanity\n",
    "                 ]\n",
    "    # drop nas\n",
    "    sims_profanity = [x for x in sims_profanity if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    # calculate avg similarities\n",
    "    sims = [np.sum(sims_insult)/len(sims_insult), np.sum(sims_abuse)/len(sims_abuse),\n",
    "           np.sum(sims_other)/len(sims_other), np.sum(sims_profanity)/len(sims_profanity)]\n",
    "\n",
    "    # choose topic with higher similarity\n",
    "    topic = [\"INSULT\", \"ABUSE\", \"OTHER\", \"PROFANITY\"][np.argmax(sims)]\n",
    "    return topic, sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58748b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess and transform new tweets\n",
    "def new_tweet_vector(tweet : str):\n",
    "    prep_new = process_tweets(tweet)\n",
    "    doc_new = nlp(prep_new)\n",
    "\n",
    "    words_new = [\n",
    "                    word.lemma_ for word in doc_new\n",
    "                    if (not word.is_stop) and (len(word)>2)\n",
    "                 ]\n",
    "\n",
    "    # calculate vector for new article\n",
    "    new_tweet_vector = get_com_vector(words_new)\n",
    "\n",
    "    return new_tweet_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede5a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create predictions with test data\n",
    "# classify_tweet(tweet_vectors, new_tweet_vector(data_test.tweet[757]))\n",
    "cat_predictions = [classify_tweet(tweet_vectors, new_tweet_vector(x))[0] for x in data_test.tweet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "acc = 0\n",
    "for i in range(len(cat_predictions)):\n",
    "    if cat_predictions[i] == data_test[\"granulareKlassifikation\"][i]:\n",
    "        acc += 1\n",
    "accuracy = acc / len(cat_predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tweet_vectors_insult = [\n",
    "                                  tweet_vectors[10]\n",
    "                                  for i in range(len(tweet_vectors))\n",
    "                                  if (data_pre[\"granulareKlassifikation\"][10]=='INSULT')\n",
    "                                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93621071",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"granulareKlassifikation\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981a5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
