{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873acbb0",
   "metadata": {},
   "source": [
    "### Download german spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b23a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5c841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/jannis/opt/miniconda3/lib/python3.9/site-packages (from fasttext) (58.0.4)\n",
      "Requirement already satisfied: numpy in /Users/jannis/opt/miniconda3/lib/python3.9/site-packages (from fasttext) (1.22.4)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-macosx_12_0_arm64.whl size=276277 sha256=5346ebabac3626f7c77609d145c7374e1a725e876a81962d206ce9ee10b5db1f\n",
      "  Stored in directory: /Users/jannis/Library/Caches/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/Users/jannis/opt/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23abb67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannis/opt/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import re\n",
    "import umap\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/jannis/ASUD_Cyberbullying/prepared_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.granulareKlassifikation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9301c1",
   "metadata": {},
   "source": [
    "## Wordclouds for every Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ddd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud without stopwords\n",
    "data = df\n",
    "german_stopwords = stopwords.words(\"german\")\n",
    "sw = set(STOPWORDS)\n",
    "\n",
    "# Wörter in Tweet as String\n",
    "data[\"tweets_clean\"] = [\" \".join(eval(word)) for word in data[\"Wörter in Tweet\"]] \n",
    "\n",
    "# drop unicode emojis and LBR\n",
    "new_tweet_ls = []\n",
    "for tweet in data.tweets_clean:\n",
    "    tweet_sp = tweet.split()\n",
    "    for word in tweet_sp:\n",
    "        if any(x in word for x in [\"000\", \"LBR\", \"lbr\"]) :\n",
    "            tweet = tweet.replace(word, \"\")\n",
    "        \n",
    "    new_tweet_ls.append(tweet)\n",
    "\n",
    "data[\"tweets_clean\"] = new_tweet_ls\n",
    "\n",
    "# Create a word cloud for every category\n",
    "wc_OTHER = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                     stopwords=german_stopwords).generate(str(data[data[\"binaereKlassifikation\"] == \"OTHER\"][\"tweets_clean\"].values))\n",
    "wc_OFFENSE = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data[data[\"binaereKlassifikation\"] == \"OFFENSE\"][\"tweets_clean\"].values))\n",
    "wc_insult = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data[data[\"granulareKlassifikation\"] == \"INSULT\"][\"tweets_clean\"].values))\n",
    "wc_abuse = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data[data[\"granulareKlassifikation\"] == \"ABUSE\"][\"tweets_clean\"].values))\n",
    "wc_profanity = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",\n",
    "                       stopwords=german_stopwords).generate(str(data[data[\"granulareKlassifikation\"] == \"PROFANITY\"][\"tweets_clean\"].values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad9354",
   "metadata": {},
   "source": [
    "### OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2126b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot word clouds\n",
    "plt.imshow(wc_OTHER, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud OTHER\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113216a",
   "metadata": {},
   "source": [
    "### OFFENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7604e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(wc_OFFENSE, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud OFFENSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c524147",
   "metadata": {},
   "source": [
    "### INSULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd46b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(wc_insult, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud INSULT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d1b78",
   "metadata": {},
   "source": [
    "### ABUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wc_abuse, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud ABUSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08735a3b",
   "metadata": {},
   "source": [
    "### PROFANITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wc_profanity, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud PROFANITY\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the tweets\n",
    "def process_tweets(tweet : str):\n",
    "    # all lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # remove Sonderzeichen etc (based on Sonderzeichen.txt)\n",
    "    tweet = re.sub('[^a-zA-ZäöüÄÖÜß]', \" \", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in training and test\n",
    "data_train = data.sample(round(0.75 * len(data)), random_state = 1).reset_index()\n",
    "data_test = data[~data.index.isin(data_train.index)].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ed127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data_train\n",
    "data_pre[\"tweets_clean\"] = [process_tweets(tweet) for tweet in data_pre[\"tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06592f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eccecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spacy docs with a nlp pipeline\n",
    "# https://spacy.io/usage/processing-pipelines\n",
    "tweet_docs = [nlp(tweet) for tweet in data_pre[\"tweets_clean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ea102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the generated lemmas for words that are no stopwords and have a length of more than two\n",
    "# Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced\n",
    "# form belongs to the language. This reduced form or root word is called a lemma.\n",
    "tweet_words = [\n",
    "    [\n",
    "    word.lemma_ for word in doc if ((not word.is_stop) and (len(word) >= 3))\n",
    "    ]\n",
    "    for doc in tweet_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6bfb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word2vec model (gensim)\n",
    "# https://developpaper.com/gensim-model-parameters-of-word2vec/\n",
    "# Window: refers to the window size of training. 8 means that the first 8 words and the last 8 words are considered \n",
    "word2vec = Word2Vec(tweet_words, min_count=3, sg=1, hs=0, negative=9,\n",
    "                    ns_exponent=0.69, window=6, vector_size=60, epochs=80)\n",
    "\n",
    "# train the model\n",
    "word2vec.train(tweet_words, total_examples=word2vec.corpus_count, epochs=word2vec.epochs)\n",
    "\n",
    "# maybe checkout pretrained word2vec models\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bad3ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model(\"/Users/jannis/ASUD_Cyberbullying/cc.de.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd6db1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fasttext.FastText._FastText at 0x107b694f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext.util.reduce_model(ft, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e1fb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9741767644882202, 'SPD'),\n",
       " (0.9415591359138489, 'FDP'),\n",
       " (0.9025838971138, 'CSU'),\n",
       " (0.8436816334724426, 'AfD'),\n",
       " (0.8246973156929016, 'Hessen-CDU'),\n",
       " (0.8146030902862549, 'SPD-Fraktion'),\n",
       " (0.8065652847290039, 'Landes-SPD'),\n",
       " (0.8028145432472229, 'Kreis-SPD'),\n",
       " (0.8010185360908508, 'Bundes-CDU'),\n",
       " (0.8007059097290039, 'NPD')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_nearest_neighbors('CDU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75646765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0896912 , -0.01565894, -0.08176471,  0.04066733, -0.073511  ,\n",
       "       -0.02129116,  0.08888793,  0.0268292 ,  0.06425303,  0.20570631,\n",
       "        0.09445151, -0.09627784,  0.01969391,  0.08115051, -0.12606567,\n",
       "       -0.04959214, -0.01531123,  0.01127227,  0.08746514,  0.02477118,\n",
       "       -0.07072134,  0.05173425,  0.02609763,  0.06149456, -0.02449843,\n",
       "       -0.08436476,  0.20028932,  0.08229449,  0.04297241, -0.05184186,\n",
       "       -0.01541056, -0.09896965,  0.06644775, -0.0081413 , -0.0028801 ,\n",
       "       -0.05652645, -0.04504419, -0.01369408,  0.04439357,  0.023516  ,\n",
       "        0.10248683,  0.09611171, -0.00666773,  0.07027929, -0.08583697,\n",
       "       -0.03420987,  0.0002555 ,  0.06344455, -0.06714977, -0.06024162,\n",
       "        0.12092026, -0.03786041, -0.0114253 ,  0.07172467, -0.02344412,\n",
       "        0.01341567,  0.06524836, -0.02372516, -0.01187893, -0.02400359,\n",
       "       -0.09336273,  0.0698304 ,  0.03234217, -0.03903273, -0.0622696 ,\n",
       "        0.0707662 , -0.03221697,  0.01591077,  0.01789971,  0.06736344,\n",
       "       -0.03304635, -0.02061639, -0.01889044, -0.10739391, -0.03340485,\n",
       "        0.0410893 ,  0.02856584, -0.0523207 ,  0.0579028 ,  0.03749702,\n",
       "        0.03458252,  0.01682629, -0.01397092,  0.01820113, -0.00373749,\n",
       "       -0.07429749,  0.06106392,  0.06430698,  0.01187832, -0.06024376,\n",
       "       -0.06457993, -0.05373168, -0.01606111, -0.02269405, -0.00100444,\n",
       "       -0.04876972,  0.03078388, -0.05777566,  0.02120353,  0.03010358],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_vector(\"Ferkel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9639514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test word similarities\n",
    "def test_word_sim(word_a, word_b):\n",
    "    print(f'{word_a} | {word_b}: {word2vec.wv.similarity(word_a, word_b)}')\n",
    "    \n",
    "    \n",
    "test_word_sim(\"CDU\", \"SPD\")\n",
    "test_word_sim(\"Korruption\", \"Europa\")\n",
    "test_word_sim(\"Merkel\", \"CDU\")\n",
    "test_word_sim(\"Flüchtling\", \"Terror\")\n",
    "test_word_sim(\"Witz\", \"Karte\")\n",
    "test_word_sim(\"CDU\", \"Ferkel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2 word similarities\n",
    "def test2_word_sim(word_list):\n",
    "    print(f'{word_list}: {word2vec.wv.most_similar(positive=word_list, negative=[], topn = 5)}')\n",
    "    \n",
    "test2_word_sim([\"Deutschland\", \"Merkel\"])\n",
    "test2_word_sim([\"Flüchtling\", \"Deutschland\"])\n",
    "test2_word_sim([\"Polizei\"])\n",
    "test2_word_sim([\"Feiertag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_word_sim([\"Geld\"])\n",
    "\n",
    "test2_word_sim([\"Welt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac80040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a category to every word\n",
    "category_dict = {}\n",
    "for word in word2vec.wv.index_to_key:\n",
    "    max_numb = {}\n",
    "    for cat in data_pre[\"granulareKlassifikation\"].unique():\n",
    "        count = sum([tweet.count(word.lower()) for tweet in data_pre[data_pre[\"granulareKlassifikation\"] == cat][\"tweet\"]])\n",
    "        max_numb[cat] = count   \n",
    "    category_dict[word] = max(max_numb, key = max_numb.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a547ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a category to every word (percentage)\n",
    "category_dict_per = {}\n",
    "for word in word2vec.wv.index_to_key:\n",
    "    max_numb = {}\n",
    "    for cat in data_pre[\"granulareKlassifikation\"].unique():\n",
    "        count = sum([tweet.count(word.lower()) for tweet in data_pre[data_pre[\"granulareKlassifikation\"] == cat][\"tweet\"]]) / len(data_pre[data_pre[\"granulareKlassifikation\"] == cat][\"tweet\"])\n",
    "        max_numb[cat] = count   \n",
    "    category_dict_per[word] = max(max_numb, key = max_numb.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32982aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution (count)\n",
    "from collections import Counter\n",
    "category_dict\n",
    "Counter(category_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution (percentage)\n",
    "from collections import Counter\n",
    "category_dict\n",
    "Counter(category_dict_per.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre[\"granulareKlassifikation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the word vectors\n",
    "# reduce to two dimensions\n",
    "reducer = umap.UMAP(metric='cosine', n_components=2, n_neighbors=15, min_dist=0.00, random_state=0)\n",
    "\n",
    "# X holds the word vectors for all words in vocabulary\n",
    "X = word2vec.syn1neg\n",
    "\n",
    "# transform to two dimensions\n",
    "embedding = reducer.fit_transform(X[:,:])\n",
    "\n",
    "# plot projected word vectors\n",
    "plt.figure(figsize=(4,4), dpi=300)\n",
    "\n",
    "# scatterplot of projected word vectors\n",
    "colors = {\"INSULT\": '#eb4034',\n",
    "          \"OTHER\": \"#dbd635\",\n",
    "          \"ABUSE\": '#34b1eb',\n",
    "          \"PROFANITY\": \"#52ab32\"}\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c = [colors.get(cat, \" \") for cat in list(category_dict_per.values())[:]],\n",
    "    s = 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we should plot this with 3 dimensions \n",
    "# Project into 3D space\n",
    "reducer3D = umap.UMAP(metric='cosine', n_components=3, n_neighbors=15, min_dist=0.00, random_state=0)\n",
    "embedding3D = reducer3D.fit_transform(X[:,:])\n",
    "\n",
    "\n",
    "# plot in 3D and create animation (rotate space)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(4,4), dpi=100)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(\n",
    "    embedding3D[:, 0],\n",
    "    embedding3D[:, 1],\n",
    "    embedding3D[:, 2],\n",
    "    c = [colors.get(cat, \" \") for cat in list(category_dict.values())[:]],\n",
    "    s = 2\n",
    "    )\n",
    "for color in ['#eb4034',\"#dbd635\",'#34b1eb',\"#52ab32\"]:\n",
    "    lbl = list(colors.keys())[list(colors.values()).index(color)]\n",
    "    plt.scatter([],[], [], color=color, label=lbl)\n",
    "plt.legend()\n",
    "\n",
    "def rotate(angle):\n",
    "    ax.view_init(azim=angle)\n",
    "\n",
    "rot_animation = animation.FuncAnimation(fig, rotate, frames=np.arange(0, 362,10), repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db6f91",
   "metadata": {},
   "source": [
    "## Classify new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d17d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate center of mass vector for list of words \n",
    "def get_com_vector(words : list) -> np.array:\n",
    "    # list of words in the word2vec model\n",
    "    words = [word for word in words if word in word2vec.wv.index_to_key]\n",
    "    # get the vectors\n",
    "    vectors = np.array([word2vec.wv.get_vector(word) for word in words])\n",
    "    # return the sum of all vectors devided by the amount of words from words in the model\n",
    "    vector = np.sum(vectors, axis=0)\n",
    "    return vector / len(words)\n",
    "\n",
    "# get vector for each tweet\n",
    "tweet_vectors = []\n",
    "for tweet in tweet_words:\n",
    "    vec = get_com_vector(tweet)\n",
    "    tweet_vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55589bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similartiy between two vectors\n",
    "def cos_sim(vec0: np.array, vec1: np.array) -> float:\n",
    "    return np.dot(vec0, vec1)/(np.linalg.norm(vec0)*np.linalg.norm(vec1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85154680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a vector by comparing cosine similarity to known article vectors\n",
    "def classify_tweet(tweet_vectors: np.array, new_tweet_v: np.array):\n",
    "\n",
    "    # split tweet_vectors into categorys\n",
    "    tweet_vectors_insult = [\n",
    "                                  tweet_vectors[i]\n",
    "                                  for i in range(len(tweet_vectors))\n",
    "                                  if (data_pre[\"granulareKlassifikation\"][i]=='INSULT')\n",
    "                                ]\n",
    "\n",
    "    tweet_vectors_abuse = [\n",
    "                               tweet_vectors[i]\n",
    "                               for i in range(len(tweet_vectors))\n",
    "                               if (data_pre[\"granulareKlassifikation\"][i]=='ABUSE')\n",
    "                            ]\n",
    "    \n",
    "    tweet_vectors_other = [\n",
    "                               tweet_vectors[i]\n",
    "                               for i in range(len(tweet_vectors))\n",
    "                               if (data_pre[\"granulareKlassifikation\"][i]=='OTHER')\n",
    "                            ]\n",
    "    tweet_vectors_profanity = [\n",
    "                               tweet_vectors[i]\n",
    "                               for i in range(len(tweet_vectors))\n",
    "                               if (data_pre[\"granulareKlassifikation\"][i]=='PROFANITY')\n",
    "                            ]\n",
    "    # calculate similarities between new_article_vector and known\n",
    "    # article_vectors for all categories\n",
    "    sims_insult = [\n",
    "                    cos_sim(new_tweet_v, av)\n",
    "                    for av in tweet_vectors_insult\n",
    "                ]\n",
    "    \n",
    "    # drop nas\n",
    "    sims_insult = [x for x in sims_insult if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    sims_abuse = [\n",
    "                    cos_sim(new_tweet_v, av)\n",
    "                    for av in tweet_vectors_abuse\n",
    "                 ]\n",
    "    # drop nas\n",
    "    sims_abuse = [x for x in sims_abuse if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    sims_other = [\n",
    "                    cos_sim(new_tweet_v, av)\n",
    "                    for av in tweet_vectors_other\n",
    "                 ]\n",
    "    # drop nas\n",
    "    sims_other = [x for x in sims_other if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    sims_profanity = [\n",
    "                    cos_sim(new_tweet_v, av)\n",
    "                    for av in tweet_vectors_profanity\n",
    "                 ]\n",
    "    # drop nas\n",
    "    sims_profanity = [x for x in sims_profanity if not hasattr(x, \"__len__\")]\n",
    "    \n",
    "    # calculate avg similarities\n",
    "    sims = [np.sum(sims_insult)/len(sims_insult), np.sum(sims_abuse)/len(sims_abuse),\n",
    "           np.sum(sims_other)/len(sims_other), np.sum(sims_profanity)/len(sims_profanity)]\n",
    "\n",
    "    # choose topic with higher similarity\n",
    "    topic = [\"INSULT\", \"ABUSE\", \"OTHER\", \"PROFANITY\"][np.argmax(sims)]\n",
    "    return topic, sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58748b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess and transform new tweets\n",
    "def new_tweet_vector(tweet : str):\n",
    "    prep_new = process_tweets(tweet)\n",
    "    doc_new = nlp(prep_new)\n",
    "\n",
    "    words_new = [\n",
    "                    word.lemma_ for word in doc_new\n",
    "                    if (not word.is_stop) and (len(word)>2)\n",
    "                 ]\n",
    "\n",
    "    # calculate vector for new article\n",
    "    new_tweet_v = get_com_vector(words_new)\n",
    "\n",
    "    return new_tweet_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede5a08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create predictions with test data\n",
    "# classify_tweet(tweet_vectors, new_tweet_vector(data_test.tweet[757]))\n",
    "cat_predictions = [classify_tweet(tweet_vectors, new_tweet_vector(x))[0] for x in data_test.tweets_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "acc = 0\n",
    "for i in range(len(cat_predictions)):\n",
    "    if cat_predictions[i] == data_test[\"granulareKlassifikation\"][i]:\n",
    "        acc += 1\n",
    "accuracy = acc / len(cat_predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93621071",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"granulareKlassifikation\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts of predictions\n",
    "[cat_predictions.count(x) for x in ['ABUSE', 'PROFANITY','INSULT', 'OTHER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"granulareKlassifikation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f6887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
